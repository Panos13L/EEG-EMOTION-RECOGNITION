{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5502e405",
   "metadata": {},
   "source": [
    "##### Executive Summary\n",
    "\n",
    "In this notebook, we train Machine Learning (ML) models on the SEED dataset using augmented features (obtained via MATLAB codes).\n",
    "\n",
    "Each model in ML and DL pipeline has been evaluated via five fold (stratified) cross validation. Then the best model is selected and its performance on the test set is evaluated. Note that the test set is fixed accross pipelines and corresponds to the 20% of the data. F1 scores of Machine Learning models on the test set is shared on the table below:\n",
    "\n",
    "| Model | Test F1 Score (%) |\n",
    "| ----- | --------------- |\n",
    "| MLP | 66.3 |\n",
    "| RF | 67.6 |\n",
    "| KNN | 67.6 |\n",
    "| Log - Reg | 77.6 |\n",
    "| SVM | 77.7 |\n",
    "| XGBoost | 79.8 |\n",
    "\n",
    "According to our experiments, XGBoost algorithm outperformed the remainining ML pipelines. Here note that each ML model has been saved under the directory `models/` by the dataset's name and model's name (e.g. knn). Also, you can find cross validation results under the `searchs/` directory.\n",
    "\n",
    "In orted to advance the performance of our models, we have trained a CNN_LSTM on the 10-timestep averaged raw data and extracted the 15992 dimensional embeddings. Note that these embeddings are optimized to model the discrepancy of each class. Then we have concatenated the 16150 dimensional MATLAB features with 15992 dimensional optimized embeddings. The resulting 32142 dimensional feature vectors are given as input to ML models. We name the models trained using this \"augmented\" approach CNN_LSTM based models. Below we share the performance of the augmented models:\n",
    "\n",
    "| Model (Combined features) | Test F1 Score (%) |\n",
    "| ----- | --------------- |\n",
    "| MLP | 82.5 |\n",
    "| RF | 80.3 |\n",
    "| KNN | 71 |\n",
    "| Log - Reg | 88.7 |\n",
    "| SVM | 85.8 |\n",
    "| XGBoost | 89.2 |\n",
    "\n",
    "Augmented models and their corresponding searchs can be found on `models/` and `searchs/` directory with the prefix `seed-combined`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a19e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.pipeline import make_pipeline, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679492be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdd16fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "gpu = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6345ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2006e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db8e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_lstm_model(T, F, num_feats=16, num_layers=2, p=0.5, num_labels=1, act='sigmoid'):\n",
    "    inp = keras.Input(shape=(T, F))\n",
    "    for l in range(num_layers): \n",
    "        x = layers.BatchNormalization(axis=-1)(x if l else inp)\n",
    "        x = layers.Conv1D(num_feats,2, activation='relu')(x)\n",
    "        x = layers.Conv1D(num_feats,2, activation='relu')(x)\n",
    "        x = layers.Dropout(p)(x)\n",
    "        x = layers.MaxPooling1D(2, data_format=\"channels_last\")(x)\n",
    "        x = layers.LSTM(100, return_sequences=True)(x)  # Return sequences for the next LSTM\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(64)(x)\n",
    "        x = layers.Dropout(0.25)(x)\n",
    "        x = layers.Dense(8,activation='relu')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    out = layers.Dense(num_labels, activation=act)(x)\n",
    "    return keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "def teleport(data):\n",
    "    mel = Spectrogram(n_fft=16, normalized=True)\n",
    "    R = mel(torch.Tensor(data.transpose([0, 2, 1]))).numpy()\n",
    "    N, F1, F2, T = R.shape\n",
    "    return R.reshape(N, F1*F2, T)\n",
    "\n",
    "def cv_keras(model, X, y, n=5, verbose=0):\n",
    "    skf = StratifiedKFold(random_state=0, n_splits=n, shuffle=True)\n",
    "    logs = {}\n",
    "    score = 0\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X, y.argmax(axis=-1))):\n",
    "        history = model.fit(X[train_index], \n",
    "                            y[train_index], \n",
    "                            validation_data=(X[test_index], y[test_index]), \n",
    "                            epochs=50, \n",
    "                            shuffle=True,\n",
    "                            callbacks=[\n",
    "                                tf.keras.callbacks.EarlyStopping(patience=5),\n",
    "                                tf.keras.callbacks.ReduceLROnPlateau(patience=3)\n",
    "                            ],\n",
    "                            verbose=verbose)\n",
    "\n",
    "        score += np.mean(history.history[\"val_f1_score\"][-2:]) / n\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cb1900",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1_Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.f1 = self.add_weight(name='f1', initializer='zeros')\n",
    "        self.precision_fn = keras.metrics.Precision(thresholds=0.5)\n",
    "        self.recall_fn = keras.metrics.Recall(thresholds=0.5)\n",
    "\n",
    "    def update_state(self, y_atrue, y_pred, sample_weight=None):\n",
    "        p = self.precision_fn(y_true, y_pred)\n",
    "        r = self.recall_fn(y_true, y_pred)\n",
    "        # since f1 is a variable, we use assign\n",
    "        self.f1.assign(2 * ((p * r) / (p + r + 1e-6)))\n",
    "\n",
    "    def result(self):\n",
    "        return self.f1\n",
    "\n",
    "    def reset_states(self):\n",
    "        # we also need to reset the state of the precision and recall objects\n",
    "        self.precision_fn.reset_states()\n",
    "        self.recall_fn.reset_states()\n",
    "        self.f1.assign(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aa9e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "path= \"C:\\\\Users\\\\loitp\\\\Downloads\\\\ftr_arr.mat\"\n",
    "data = loadmat(path)['ftr_arr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c34d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=np.array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4035b90e",
   "metadata": {},
   "source": [
    "## 1. Label Balance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d6f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Label balance:\\nClass -1: {round((labels == -1).sum() / len(labels), 2)}\\nClass  0: {round((labels == 0).sum() / len(labels), 2)}\\nClass  1: {round((labels == 1).sum() / len(labels), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5855e1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TYPE = \"seed-feats\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4922379",
   "metadata": {},
   "source": [
    "# 2. Training Machine Learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0005fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(data, labels,\n",
    "                                                test_size=0.2,\n",
    "                                                stratify=labels,\n",
    "                                                random_state =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854ea1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() # Scaling the data. Use another scaling function if you want.\n",
    "Xtr = scaler.fit_transform(Xtrain)\n",
    "Xte = scaler.fit_transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, F = Xtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2779e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    ('logreg', [('clf', LogisticRegression(random_state=0, max_iter=1000))]),\n",
    "    ('logreg-pca', [('pca', PCA(random_state=0)), ('clf', LogisticRegression(random_state=0, max_iter=1000))]),\n",
    "    ('knn', [('clf', KNeighborsClassifier())]),\n",
    "    ('knn-pca', [('pca', PCA(random_state=0)), ('clf', KNeighborsClassifier())]),\n",
    "    ('svm', [('clf', SVC(random_state=0))]),\n",
    "    ('svm-pca', [('pca', PCA(random_state=0)), ('clf', SVC(random_state=0))]),\n",
    "    ('neuralnet', [('clf', MLPClassifier(random_state=0, learning_rate=\"adaptive\", max_iter=1000, early_stopping=True))]),\n",
    "    ('neuralnet-pca', [('pca', PCA(random_state=0)), ('clf', MLPClassifier(random_state=0, learning_rate=\"adaptive\", max_iter=1000, early_stopping=True))]),\n",
    "    ('rf', [('clf', RandomForestClassifier(random_state=0))]),\n",
    "    ('rf-pca', [('pca', PCA(random_state=0)), ('clf', RandomForestClassifier(random_state=0))])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276606cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = [\n",
    "    {\"clf__C\": np.logspace(-6, 0, 10)},\n",
    "    {\"pca__n_components\": [5, 10, 25, 50, 100], \"clf__C\": np.logspace(-6, 0, 10)},\n",
    "    {\"clf__n_neighbors\": [2, 5, 10, 25, 50]},\n",
    "    {\"pca__n_components\": [5, 10, 25, 50, 100], \"clf__n_neighbors\": [2, 5, 10, 25, 50]},\n",
    "    {\"clf__C\": np.logspace(-6, 0, 10), \"clf__kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]},\n",
    "    {\"pca__n_components\": [5, 10, 25, 50, 100], \"clf__C\": np.logspace(-6, 0, 10), \"clf__kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]},\n",
    "    {\"clf__hidden_layer_sizes\": [*product([8, 16, 32, 64], [8, 16, 32, 64])], 'clf__learning_rate_init': np.logspace(-4, 0, 4), 'clf__alpha': np.logspace(-4, 0, 4)},\n",
    "    {\"pca__n_components\": [5, 10, 25, 50, 100], \"clf__hidden_layer_sizes\": [*product([8, 16, 32, 64], [8, 16, 32, 64])], 'clf__learning_rate_init': np.logspace(-4, 0, 4), 'clf__alpha': np.logspace(-4, 0, 4)},\n",
    "    {\"clf__n_estimators\": [10, 25, 50, 100, 200]},\n",
    "    {\"pca__n_components\": [5, 10, 25, 50, 100], \"clf__n_estimators\": [10, 25, 50, 100, 200], \"clf__max_depth\": [2, 3, 5, None], \"clf_min_samples_leaf\": [1, 3, 5, 10, 20]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48055992",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"...\\\\models{DATA_TYPE}-scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29140a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for (name, pipe), grid in zip(steps, grids):\n",
    "    if name.endswith('-pca'):\n",
    "        continue\n",
    "    print(f\"@{name}\")\n",
    "    pipe = [('selector', SelectKBest(mutual_info_classif))] + pipe\n",
    "    grid['selector__k'] = np.logspace(0.7, np.log10(F), 5).astype(int)\n",
    "    pipeline = Pipeline(pipe)\n",
    "    grid = GridSearchCV(pipeline, grid, cv=5, scoring=\"f1_macro\", n_jobs=-1)\n",
    "    grid.fit(Xtr, ytrain)\n",
    "    pd.DataFrame(grid.cv_results_).sort_values(\"rank_test_score\").to_csv(f\"...\\\\searchs{DATA_TYPE}-{name}.csv\")\n",
    "    clf = grid.best_estimator_\n",
    "    clf.fit(Xtr, ytrain)\n",
    "    with open(f\"...\\\\models{DATA_TYPE}-{name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(clf, f)\n",
    "    y_tr_hat = clf.predict(Xtr)\n",
    "    y_te_hat = clf.predict(Xte)\n",
    "    res = {\"name\": name}\n",
    "    for phase, pred, true in [(\"train\", y_tr_hat, ytrain), (\"test\", y_te_hat, ytest)]:\n",
    "        for f in [accuracy_score, precision_score, recall_score, f1_score]:\n",
    "            try: \n",
    "                res[f\"{phase}-{f.__name__}\"] = f(true, pred, average=\"macro\")\n",
    "            except:\n",
    "                res[f\"{phase}-{f.__name__}\"] = f(true, pred)\n",
    "        \n",
    "    results.append(res)\n",
    "    print(json.dumps(res, indent=2))\n",
    "    print(\"-\"*50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa9ec6",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8d4dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "name = \"xgb\"\n",
    "pipe = [('clf', XGBClassifier(n_estimators=100, objective='multi:softmax', n_jobs=-1,\n",
    "                    silent=True, nthread=4))]\n",
    "grid = {\n",
    "        'clf__n_estimators': [100, 200, 300, 400, 500],\n",
    "        'clf__max_depth': [3, 4, 5],\n",
    "        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "        'min_child_weight': [1, 2, 3, 4, 5],\n",
    "        'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05],\n",
    "        'learning_rate': [0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74eea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = [('selector', SelectKBest(mutual_info_classif))] + pipe\n",
    "grid['selector__k'] = np.logspace(0.7, np.log10(F), 5).astype(int)\n",
    "pipeline = Pipeline(pipe)\n",
    "try:\n",
    "    grid = GridSearchCV(pipeline, grid, scoring=\"f1_macro\", n_jobs=-1, verbose=4)\n",
    "    grid.fit(Xtr, ytrain)\n",
    "    pd.DataFrame(grid.cv_results_).sort_values(\"rank_test_score\").to_csv(f\"searchs/{DATA_TYPE}-{name}.csv\")\n",
    "    clf = grid.best_estimator_\n",
    "    clf.fit(Xtr, ytrain)\n",
    "    with open(f\"models/{DATA_TYPE}-{name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(clf, f)\n",
    "    y_tr_hat = clf.predict(Xtr)\n",
    "    y_te_hat = clf.predict(Xte)\n",
    "    res = {\"name\": name}\n",
    "    for phase, pred, true in [(\"train\", y_tr_hat, ytrain), (\"test\", y_te_hat, ytest)]:\n",
    "        for f in [accuracy_score, precision_score, recall_score, f1_score]:\n",
    "            try: \n",
    "                res[f\"{phase}-{f.__name__}\"] = f(true, pred, average=\"macro\")\n",
    "            except:\n",
    "                res[f\"{phase}-{f.__name__}\"] = f(true, pred)\n",
    "\n",
    "    print(json.dumps(res, indent=2))\n",
    "    print(\"-\"*50)\n",
    "    print()\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992198e9",
   "metadata": {},
   "source": [
    "# 3. Deep Learning Models for Automated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3fbb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_avgd = torch.nn.AvgPool1d(10)(torch.Tensor(np.load(\"data/other/assembled_data.npy\"))).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba868cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(data_avgd, labels,\n",
    "                                                test_size=0.2,\n",
    "                                                stratify=labels,\n",
    "                                                random_state =0)\n",
    "Xtrain = np.transpose(Xtrain, [0, 2, 1])\n",
    "Xtest = np.transpose(Xtest, [0, 2, 1])\n",
    "Xtrain.shape, Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a5722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = to_categorical(ytrain, num_classes=3)\n",
    "ytest = to_categorical(ytest, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e6f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, T, F = Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dd40c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TYPE = \"seed_cnn_lstm\"\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e416389",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn_lstm_model(*Xtrain.shape[1:], num_feats=8, num_layers=1, act='softmax', num_labels=3)\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "                  loss=\"categorical_crossentropy\", \n",
    "                  metrics=[\"accuracy\", keras.metrics.Precision(),\n",
    "                           keras.metrics.Recall(), \n",
    "                           F1_Score()])\n",
    "history = model.fit(Xtrain, \n",
    "                            ytrain, \n",
    "                            validation_data=(Xtest, ytest), \n",
    "                            epochs=50, \n",
    "                            shuffle=True,\n",
    "                            callbacks=[\n",
    "                                tf.keras.callbacks.EarlyStopping(patience=5),\n",
    "                                tf.keras.callbacks.ReduceLROnPlateau(patience=3)\n",
    "                            ],\n",
    "                            verbose=1)\n",
    "model.save(f\".../models/{DATA_TYPE}-cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab1b760",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c744cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'flatten'\n",
    "intermediate_layer_model = keras.Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3ead60",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_feats = intermediate_layer_model.predict(data_avgd.transpose([0, 2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3978e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_feats = np.concatenate([data[:, :], cnn_feats], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c73da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del Xtrain\n",
    "del Xtest\n",
    "del data_avgd\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d63d7c8",
   "metadata": {},
   "source": [
    "# 4. Handcrafted Features + CNN_LSTM Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b5e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TYPE = \"seed-combined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23427916",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(combined_feats\n",
    "                                                ,\n",
    "                                                labels,\n",
    "                                                test_size=0.2,\n",
    "                                                stratify=labels,\n",
    "                                                random_state =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2486dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() # scaling the data\n",
    "Xtr = scaler.fit_transform(Xtrain)\n",
    "Xte = scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e022ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    ('logreg', [('clf', LogisticRegression(random_state=0, max_iter=1000))]),\n",
    "    ('logreg-pca', [('pca', PCA(random_state=0)), ('clf', LogisticRegression(random_state=0, max_iter=1000))]),\n",
    "    ('knn', [('clf', KNeighborsClassifier())]),\n",
    "    ('knn-pca', [('pca', PCA(random_state=0)), ('clf', KNeighborsClassifier())]),\n",
    "    ('svm', [('clf', SVC(random_state=0))]),\n",
    "    ('svm-pca', [('pca', PCA(random_state=0)), ('clf', SVC(random_state=0))]),\n",
    "    ('neuralnet', [('clf', MLPClassifier(random_state=0, learning_rate=\"adaptive\", max_iter=1000, early_stopping=True))]),\n",
    "    ('neuralnet-pca', [('pca', PCA(random_state=0)), ('clf', MLPClassifier(random_state=0, learning_rate=\"adaptive\", max_iter=1000, early_stopping=True))]),\n",
    "    ('rf', [('clf', RandomForestClassifier(random_state=0))]),\n",
    "    ('rf-pca', [('pca', PCA(random_state=0)), ('clf', RandomForestClassifier(random_state=0))])\n",
    "]\n",
    "\n",
    "grids = [\n",
    "    {\"clf__C\": np.logspace(-6, 0, 10)},\n",
    "    {\"pca__n_components\": [5, 10, 25, 50, 100], \"clf__C\": np.logspace(-6, 0, 10)},\n",
    "    {\"clf__n_neighbors\": [2]},\n",
    "    {\"pca__n_components\": [5, 10, 25, 50, 100], \"clf__n_neighbors\": [2]},\n",
    "    {\"clf__C\": np.logspace(-6, 0, 10), \"clf__kernel\": [\"linear\"]},\n",
    "    {\"pca__n_components\": [5, 10, 25, 50, 100], \"clf__C\": np.logspace(-6, 0, 10), \"clf__kernel\": [\"linear\"]},\n",
    "    {\"clf__hidden_layer_sizes\": [*product([16, 32, 64], [16, 32, 64])], 'clf__learning_rate_init': np.logspace(-4, 0, 4), 'clf__alpha': np.logspace(-4, 0, 4)},\n",
    "    {\"pca__n_components\": [5, 10, 25, 50, 100], \"clf__hidden_layer_sizes\": [*product([16, 32, 64], [16, 32, 64])], 'clf__learning_rate_init': np.logspace(-4, 0, 4), 'clf__alpha': np.logspace(-4, 0, 4)},\n",
    "    {\"clf__n_estimators\": [100, 200]},\n",
    "    {\"pca__n_components\": [5, 10, 25, 50, 100], \"clf__n_estimators\": [100, 200], \"clf__max_depth\": [2, 3, 5, None], \"clf_min_samples_leaf\": [1, 3, 5, 10, 20]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d4214",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"D:/data/other/{DATA_TYPE}-scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2ed665",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for (name, pipe), grid in zip(steps, grids):\n",
    "    if name.endswith('-pca'):\n",
    "        continue\n",
    "    print(f\"@{name}\")\n",
    "    pipe = [('selector', SelectKBest(mutual_info_classif))] + pipe\n",
    "    grid['selector__k'] = np.logspace(0.7, np.log10(F), 5).astype(int)\n",
    "    pipeline = Pipeline(pipe)\n",
    "    grid = GridSearchCV(pipeline, grid, cv=5, scoring=\"f1_macro\", n_jobs=-1)\n",
    "    grid.fit(Xtr, ytrain)\n",
    "    pd.DataFrame(grid.cv_results_).sort_values(\"rank_test_score\").to_csv(f\"D:/data/other/{DATA_TYPE}-{name}.csv\")\n",
    "    \n",
    "    clf = grid.best_estimator_\n",
    "    clf.fit(Xtr, ytrain)\n",
    "    with open(f\"D:/data/other/{DATA_TYPE}-{name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(clf, f)\n",
    "    y_tr_hat = clf.predict(Xtr)\n",
    "    y_te_hat = clf.predict(Xte)\n",
    "    res = {\"name\": name}\n",
    "    for phase, pred, true in [(\"train\", y_tr_hat, ytrain), (\"test\", y_te_hat, ytest)]:\n",
    "        for f in [accuracy_score, precision_score, recall_score, f1_score]:\n",
    "            try: \n",
    "                res[f\"{phase}-{f.__name__}\"] = f(true, pred, average=\"macro\")\n",
    "            except:\n",
    "                res[f\"{phase}-{f.__name__}\"] = f(true, pred)\n",
    "        \n",
    "    results.append(res)\n",
    "    print(json.dumps(res, indent=2))\n",
    "    print(\"-\"*50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb817372",
   "metadata": {},
   "source": [
    "### XGBOOST With Combined Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1245e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "ytrain = le.fit_transform(ytrain)\n",
    "ytest = le.fit_transform(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee5b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "name = \"xgb\"\n",
    "pipe = [('clf', XGBClassifier(n_estimators=100, objective='multi:softmax', n_jobs=-1,\n",
    "                    silent=True, nthread=4))]\n",
    "grid = {\n",
    "        'clf__n_estimators': [100, 200, 300, 400, 500],\n",
    "        'clf__max_depth': [3, 4, 5],\n",
    "        'clf__subsample': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "        'clf__colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "        'clf__min_child_weight': [1, 2, 3, 4, 5],\n",
    "        'clf__reg_alpha':[0, 0.001, 0.005, 0.01, 0.05],\n",
    "        'clf__learning_rate': [0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47c6c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = [('selector', SelectKBest(mutual_info_classif))] + pipe\n",
    "grid['selector__k'] = np.logspace(0.7, np.log10(F), 5).astype(int)\n",
    "pipeline = Pipeline(pipe)\n",
    "try:\n",
    "    grid = GridSearchCV(pipeline, grid, scoring=\"f1_macro\", n_jobs=-1, verbose=4)\n",
    "    grid.fit(Xtr, ytrain)\n",
    "    pd.DataFrame(grid.cv_results_).sort_values(\"rank_test_score\").to_csv(f\"D:/data/other/{DATA_TYPE}-{name}.csv\")\n",
    "    clf = grid.best_estimator_\n",
    "    clf.fit(Xtr, ytrain)\n",
    "    with open(f\"D:/data/other/{DATA_TYPE}-{name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(clf, f)\n",
    "    y_tr_hat = clf.predict(Xtr)\n",
    "    y_te_hat = clf.predict(Xte)\n",
    "    res = {\"name\": name}\n",
    "    for phase, pred, true in [(\"train\", y_tr_hat, ytrain), (\"test\", y_te_hat, ytest)]:\n",
    "        for f in [accuracy_score, precision_score, recall_score, f1_score]:\n",
    "            try: \n",
    "                res[f\"{phase}-{f.__name__}\"] = f(true, pred, average=\"macro\")\n",
    "            except:\n",
    "                res[f\"{phase}-{f.__name__}\"] = f(true, pred)\n",
    "    print(json.dumps(res, indent=2))\n",
    "    print(\"-\"*50)\n",
    "    print()\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
